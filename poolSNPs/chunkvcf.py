import sys, os
from typing import *
import numpy as np
import pandas as pd
import cyvcf2
import pysam
from itertools import starmap, repeat
import math
import shutil
import multiprocessing as mp
import argparse

home_dir = os.path.expanduser("~")
proj_dir = os.path.join(home_dir, '1000Genomes')
sys.path.insert(0, proj_dir)

nb_cores = os.cpu_count()

from scripts.VCFPooling.poolSNPs import parameters as prm
from scripts.VCFPooling.poolSNPs import pybcf
from scripts.VCFPooling.poolSNPs import pool
from scripts.VCFPooling.poolSNPs import dataframe as vcfdf
from scripts.VCFPooling.poolSNPs.alleles import alleles_tools as alltls

from persotools.files import delete_file, mkdir, FilePath
from persotools.struct import NamedDict

'''
Parallelized file processing: Read main vcf and write chunks
Implement for current cyvcf2-based pooling for now
Steps:
* 
*
*
'''


class PysamVariantCallGenerator(object):
    """
    Generates single-type formatted calls of variants
    """

    def __init__(self, vcfpath: FilePath, format: str = None):
        """
        :param vcfpath:
        :param indextype: identifier for variants: 'id', 'chrom:pos'.
        Must be 'chrom:pos' if the input has been generated by Phaser
        """
        self.path = vcfpath
        self.fmt = format

    def __iter__(self):
        vcfobj = pysam.VariantFile(self.path)
        for var in vcfobj:
            yield [g[self.fmt] for g in var.samples.values()]


class PysamVariantChunkGenerator(object):
    """
    Generates chunks of single-type formatted calls of variants
    """

    def __init__(self, vcfpath: FilePath, format: str = None, chunksize: int = None):
        """
        :param vcfpath:
        :param indextype: identifier for variants: 'id', 'chrom:pos'.
        Must be 'chrom:pos' if the input has been generated by Phaser
        """
        self.path = vcfpath
        self.fmt = format
        self.chksz = chunksize
        self.chrom = [*pysam.VariantFile(self.path).header.contigs][0]
        # extract chrom, works if only 1 chrom in the file
        self.pack = True
        self.newpos = 1  # for valid self.newpos - 1 at the start of the first chunk

    def chunk(self, chunksize: int, newpos: int):
        """Build generators of variants calls"""
        iterator = pysam.VariantFile(self.path)
        try:
            for i, v in enumerate(iterator.fetch(contig=self.chrom, start=newpos - 1, reopen=False)):
                # newpos - 1: avoids first variant truncation in the next chunk
                var = v
                if i == chunksize:
                    break
                yield var

        except StopIteration:
            print('Could not build chunk')

    def incrementer(self, chunksize: int):
        """update position and packing bool"""
        iterator = pysam.VariantFile(self.path)
        try:
            for i, v in enumerate(iterator.fetch(contig=self.chrom, start=self.newpos - 1, reopen=False)):
                # self.newpos - 1: avoids first variant truncation in the next chunk
                var = v
                if i == chunksize:
                    self.newpos = var.pos
                    break
            if var.pos != self.newpos:  # reached EOF
                self.pack = False

        except StopIteration:
            self.newpos = None
            self.pack = False

        finally:
            return self.newpos, self.pack

    def chunkpacker(self):
        while self.pack:
            chk = self.chunk(self.chksz, self.newpos)
            # function output and included sttributes updates NOT unpacked hence NOT updated
            self.newpos, self.pack = self.incrementer(self.chksz)
            yield chk
            print(self.newpos, self.pack)


class PysamChunkHandler(object):
    """
    what I will be able to do with chunks of variants:
    * track header of the main file, update header if necessary
    * process e.g. simualte pooling
    * write back to vcf
    """
    def __init__(self, mainpath: FilePath, packedchunk):
        self.mainf = pysam.VariantFile(mainpath)
        self.data = packedchunk

    @property
    def header(self):
        return self.mainf.header

    def process(self):
        # any way to update a record?
        for rec in self.data:
            var = rec
            for v in var.samples.values():
                v['GT'] = (None, None)
            yield var

    def writechunk(self, pathout: FilePath, data):
        """
        :param data: variant generator
        """
        fout = pysam.VariantFile(pathout, 'w', header=self.header)
        data = self.process()
        for rec in data:
            var = rec
            print([v['GT'] for v in var.samples.values()])
            fout.write(rec)
        fout.close()


class PysamVariantPooler(object):
    """
    Class translation of pool.process_line
    """
    def __init__(self, groups: list, simul: str, format: str,
                 w: pysam.VariantFile, v: pysam.VariantRecord,
                 dict_gl: dict,  write: bool = True):
        self.groups = groups
        self.simul = simul
        self.fmt = format
        self.writer = w
        self.record = v
        self.lookup = dict_gl
        self._write = write
        self.samples = self.record.samples.keys()
        self.genotypes = np.asarray(self.record.samples.values()[self.fmt])
        self.pooled_record = self.record.copy()
        self.blocks = []
        for gp in groups[0]:
            self.blocks.append(pool.SNPsPool().set_subset(gp))

    def simulate_pooling(self):
        self.pooled_record.format = prm.GTGL
        for p in self.blocks:
            p.set_line_values(self.samples, self.record)
            if prm.GTGL == 'GL' and prm.unknown_gl == 'adaptive':
                self.pooled_record = p.decode_genotypes_gl(self.genotypes, self.lookup)
            else:  # prm.GTGL == 'GT' or fixed GL
                self.pooled_record = p.decode_genotypes_gt(self.genotypes)

    def simulate_rdmissing(self):
        pass

    def write_variant(self):
        pass


class PysamFilePooler(object):
    """
    Class translation of pool.process_file
    modify header if GT pooled to GL
    """
    def __init__(self, data: pysam.VariantFile, groups: list, simul: str):
        self.data = data
        self.groups = groups
        self.simul = simul

    def adapt_gl(self):
        pass

    def process(self):
        pass


class CyvcfVariantCallGenerator(object):
    """
    Generates single-type formatted calls of variants
    """

    def __init__(self, vcfpath: FilePath):
        """
        :param vcfpath:
        :param pos: access variants from pos position
        """
        self.path = vcfpath
        self.chrom = [*cyvcf2.VCF(self.path)][0].CHROM

    def __iter__(self, pos: str = None):
        if pos is None:
            vcfobj = cyvcf2.VCF(self.path)
        else:
            f = cyvcf2.VCF(self.path)
            vcfobj = f('{}:{}'.format(self.chrom, pos))
        return vcfobj


class CyvcfVariantChunkGenerator(object):
    """
    Generates chunks of single-type formatted calls of variants
    Based on cyvcf2 library
    """

    def __init__(self, vcfpath: FilePath, chunksize: int = None):
        """
        :param vcfpath:
        :param indextype: identifier for variants: 'id', 'chrom:pos'.
        Must be 'chrom:pos' if the input has been generated by Phaser
        """
        self.path = vcfpath
        self.chksz = chunksize
        self.chrom = [*cyvcf2.VCF(self.path)][0].CHROM
        # extract chrom, works if only 1 chrom in the file
        self.pack = True
        self.newpos = 1  # for valid self.newpos - 1 at the start of the first chunk

    def chunker(self, chunksize: int, newpos: int):
        """Build generators of variants calls"""
        iterator = cyvcf2.VCF(self.path)
        try:
            for i, v in enumerate(iterator('{}:{}'.format(self.chrom, newpos - 1))):
                # newpos - 1: avoids first variant truncation in the next chunk
                var = v
                if i == chunksize:
                    break
                yield var

        except StopIteration:
            print('Could not build chunk')

    def incrementer(self, chunksize: int):
        """update position and packing bool"""
        iterator = cyvcf2.VCF(self.path)
        try:
            for i, v in enumerate(iterator('{}:{}'.format(self.chrom, self.newpos - 1))):
                # self.newpos - 1: avoids first variant truncation in the next chunk
                var = v
                if i == chunksize:
                    self.newpos = var.POS  # POS in cap. letters with cyvcf2
                    break
            if var.POS != self.newpos:  # reached EOF
                self.pack = False

        except StopIteration:
            self.newpos = None
            self.pack = False

        finally:
            return self.newpos, self.pack

    def chunkpacker(self):
        while self.pack:
            chk = self.chunker(self.chksz, self.newpos)
            # function output and included sttributes updates NOT unpacked hence NOT updated
            self.newpos, self.pack = self.incrementer(self.chksz)
            yield chk
            # print(self.newpos, self.pack)

    def chunkwriter(self, chki: int = None, prefix=None, bgz: bool = False):
        data = [*self.chunkpacker()][chki]
        baspath = os.path.basename(self.path).rstrip('.gz')
        pathout = os.path.join(prm.TMP_DATA_PATH,
                               '{}{}.{}'.format(prefix, chki, baspath))
        w = cyvcf2.Writer(pathout, cyvcf2.VCF(self.path))
        for var in data:
            w.write_record(var)
        w.close()
        if bgz:
            pybcf.bgzip('{}{}.{}'.format(prefix, chki, baspath),
                        '{}{}.{}'.format(prefix, chki, baspath) + '.gz',
                        prm.TMP_DATA_PATH)


class CyvcfChunkHandler(object):
    """
    what I will be able to do with chunks of variants:
    * track header of the main file, update header if necessary
    * process e.g. simualte pooling
    * write back to vcf: 1 vcf in, 1 vcf out
    """
    def __init__(self, mainpath: FilePath, groups: list = None, gdict: dict = None, fileout: FilePath = None,
                 packedchunk: CyvcfVariantChunkGenerator.chunker = None):
        self.mainf = cyvcf2.VCF(mainpath)
        if packedchunk is not None:
            self.data = packedchunk
        else:
            self.data = self.mainf
        self.groups = groups
        self.gdict = gdict
        self.filout = fileout

    @property
    def header(self):
        return self.mainf.raw_header

    def add_format_to_header(self, line=None):
        """
        :param line: dict containing keys for ID, Number, Type, Description.
        """
        str_header = '##FORMAT=<ID=GL,Number=G,Type=Float,Description="three log10-scaled likelihoods for RR,RA,AA genotypes">'
        dic_header = {'ID': 'GL',
                      'Number': 'G',
                      'Type': 'Float',
                      'Description': 'three log10-scaled likelihoods for RR,RA,AA genotypes'}
        self.mainf.add_format_to_header(dic_header)

    def write_header(self):
        self.add_format_to_header()
        fout = cyvcf2.Writer(self.filout, self.mainf)
        fout.write_header()
        fout.close()

    def process(self):
        if False:  # write GT only format
            fout = cyvcf2.Writer(self.filout, self.mainf, mode='w')
        else:  # write other formats
            self.write_header()
            fout = open(self.filout, 'ab')
        for rec in self.data:
            var = rec
            # pool.process_line(self.groups, 'pooled', fout, var, self.gdict, write=True)
            cvp = CyvcfVariantPooler(self.groups, 'pooled', fout, var, self.gdict, write=True)
            cvp.simulate_pooling()
            cvp.write_variant()
            break
        fout.close()

    def write_chunk(self):
        """
        :param data: variant generator
        """
        fout = cyvcf2.Writer(self.filout, self.mainf)
        fout.close()


def _cyvcfchunkhandler_process(*arglist):
    cch = CyvcfChunkHandler(*arglist)
    cch.process()


class CyvcfVariantPooler(object):
    """
    Class translation of pool.process_line
    """
    def __init__(self, groups: list, simul: str,
                 w: cyvcf2.Writer, v: cyvcf2.Variant,
                 gdict: dict,  write: bool = True) -> None:
        self.groups = groups
        self.simul = simul
        self.writer = w
        self.record = v
        self.lookup = gdict
        self._write = write
        self.samples = cyvcf2.VCF(self.writer.name).samples
        self.genotypes = np.asarray(self.record.genotypes)
        self.pooled_record = self.record
        self.blocks = [pool.SNPsPool().set_subset(np.asarray(gp)) for gp in self.groups[0]]
        # for gp in groups[0]:
        #     self.blocks.append(pool.SNPsPool().set_subset(gp))

    def simulate_pooling(self):
        for p in self.blocks:
            p.set_line_values(self.samples, self.record)
            if prm.GTGL == 'GL' and prm.unknown_gl == 'adaptive':
                self.pooled_record = p.decode_genotypes_gl(self.genotypes, self.lookup)
            else:  # prm.GTGL == 'GT' or fixed GL
                self.pooled_record = p.decode_genotypes_gt(self.genotypes)

    def simulate_rdmissing(self):
        for p in self.blocks:
            p.set_line_values(self.samples, self.record)
            dlt = pool.random_delete(activate=True)
            idx = np.argwhere(np.isin(self.samples, p))
            if dlt:
                if prm.GTGL == 'GL' and prm.unknown_gl == 'adaptive':
                    self.pooled_record = self.pooled_record.astype(float)  # avoid truncating GL
                    np.put(self.pooled_record, idx, np.asarray([1 / 3, 1 / 3, 1 / 3]))
                else:
                    np.put(self.pooled_record, idx, np.asarray([-1, -1, 0]))

    def write_variant(self):
        if prm.GTGL == 'GL' and prm.unknown_gl == 'adaptive':
            # cyvcf2.Variant.genotypes does not handle GL-format
            # customize line format for GL
            logzero = np.vectorize(lambda x: -5.0 if x <= pow(10, -5) else math.log10(x))
            info = ';'.join([kv for kv in ['='.join([str(k), str(v)]) for k, v in self.record.INFO]])
            gl = alltls.repr_gl_array(logzero(self.pooled_record))
            toshow = np.asarray([self.record.CHROM,
                                 self.record.POS,
                                 self.record.ID,
                                 ''.join(self.record.REF),
                                 ''.join(self.record.ALT),
                                 self.record.QUAL if not None else '.',
                                 'PASS' if self.record.FILTER is None else self.record.FILTER,
                                 info,
                                 'GL',
                                 gl],
                                dtype=str)
            towrite = '\t'.join(toshow) + '\n'
            stream = towrite.encode()
            self.writer.write(stream)  # cyvcf2.Writer.variant_from_string() does not write anything
        else:
            # cyvcf2.Variant.genotypes does handle GT-format
            self.record.genotypes = self.pooled_record.tolist()
            self.writer.write_record(self.record)


def cyvcf_parallel_pooling(pth: FilePath, chkdata: CyvcfVariantChunkGenerator.chunker, groups: list, gdict: dict) -> None:
    handler = CyvcfChunkHandler(pth, chkdata)
    handler.process(splits, df2dict)


if __name__ == '__main__':
    os.chdir('/home/camille/1000Genomes/data/gl/gl_adaptive/all_snps_all_samples')
    gtglpth = 'IMP.chr20.pooled.beagle2.gl.chunk10000.corr.vcf.gz'
    gtpth = '/home/camille/1000Genomes/data/gt/ALL.chr20.snps.gt.chunk10000.vcf.gz'

    pysamobj = vcfdf.PandasMixedVCF(gtglpth, format='GP')
    pysamvar = PysamVariantCallGenerator(gtglpth, format='GP')
    pysamchunk = PysamVariantChunkGenerator(gtglpth, format='GP', chunksize=1000)

    cyvcfobj = vcfdf.PandasVCF(gtpth)
    cyvcfvar = CyvcfVariantCallGenerator(gtpth)
    cyvcfchunk = CyvcfVariantChunkGenerator(gtpth, chunksize=1000)

    df = pd.read_csv(os.path.join(prm.WD, 'adaptive_gls.csv'),
                     header=None,
                     names=['rowsrr', 'rowsra', 'rowsaa', 'colsrr', 'colsra', 'colsaa',
                            'n', 'm',
                            'rr', 'ra', 'aa']
                     )
    df2dict = dict(((int(rwrr), int(rwra), int(rwaa), int(clrr), int(clra), int(claa),
                     int(n), int(m)),
                    [rr, ra, aa]) for rwrr, rwra, rwaa, clrr, clra, claa,
                                      n, m,
                                      rr, ra, aa in df.itertuples(index=False, name=None))

    raw = cyvcf2.VCF(os.path.join(prm.WD, 'gt', prm.SRCFILE))  # VCF iterator object
    splits = pool.split_pools(raw.samples, 16, seed=123)  # list of lists

    chunkpack = pysamchunk.chunkpacker()
    for i, chk in enumerate(chunkpack):
        break
        # print(i, len([*chk]))  # empties chk!
        print('\r\n', i)
        chki = PysamChunkHandler(mainpth, chk)
        chki.writechunk('chktest{}.vcf'.format(i), chki.data)
        break

    cyvcfpack = cyvcfchunk.chunkpacker()
    # for i, chk in enumerate(cyvcfpack):
    #      # print(i, len([*chk]))
    #      chkdata = chk
    #      break
    #
    # cyvcfhandler = CyvcfChunkHandler(gtpth, chkdata, groups=splits, gdict=df2dict)
    # cyvcfhandler.process()  # processes 1 chunk

    # must write chunks to files before further processing
    prename = 'pack'
    indices = np.arange(len([*cyvcfpack]))
    args0 = list(zip(indices,
                     repeat(prename, len(indices))))
    # with mp.Pool(processes=os.cpu_count()) as mpool:
    #    _ = all(mpool.starmap(CyvcfVariantChunkGenerator(gtpth, chunksize=1000).chunkwriter, args0))

    files0 = os.listdir(prm.TMP_DATA_PATH)
    files1 = ['pooled.{}'.format(f0) for f0 in files0]
    args1 = list(zip(files0,
                     repeat(splits, len(indices)),
                     repeat(df2dict, len(indices)),
                     files1))
    os.chdir(prm.TMP_DATA_PATH)
    cch = _cyvcfchunkhandler_process(*args1[0])  # processes 1 chunked file
    # with mp.Pool(processes=os.cpu_count()//4) as mpool:
    #      _ = all(mpool.starmap(_cyvcfchunkhandler_process, args1))
