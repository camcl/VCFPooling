import sys, os
from typing import *
import numpy as np
import pandas as pd
import cyvcf2
import pysam
from itertools import repeat
import math
import multiprocessing as mp

home_dir = os.path.expanduser("~")
proj_dir = os.path.join(home_dir, '1000Genomes/scripts')
sys.path.insert(0, proj_dir)

nb_cores = os.cpu_count()

from VCFPooling.poolSNPs import parameters as prm
from VCFPooling.poolSNPs import pybcf
from VCFPooling.python.archived import pool
from VCFPooling.python.archived.alleles import alleles_tools as alltls

from VCFPooling.persotools.files import delete_file, FilePath

'''
Classes for parallelized file processing: Read main VCF-file and write chunks from it
1) Implementation based on cyvcf2 file handling (e.g. pooling)
2) Implementation based on pysam file handling (SNPsPool to reimplement to rewrite)

Steps:
* Read main VCF-file: iterate over variants and repack them into chunks
* Write packed chunks to independent VCF-files
* Apply pooling simulation to a population at one marker
* Scale up pooling to a population over all variants in a VCF-file
'''


class PysamVariantCallGenerator(object):
    """
    Generates single-type formatted calls of variants
    """

    def __init__(self, vcfpath: FilePath, format: str = None):
        """
        :param vcfpath:
        :param indextype: identifier for variants: 'id', 'chrom:pos'.
        Must be 'chrom:pos' if the input has been generated by Phaser
        """
        self.path = vcfpath
        self.fmt = format

    def __iter__(self):
        vcfobj = pysam.VariantFile(self.path)
        for var in vcfobj:
            yield [g[self.fmt] for g in var.samples.values()]


class PysamVariantChunkGenerator(object):
    """
    Generates chunks of single-type formatted calls of variants
    """

    def __init__(self, vcfpath: FilePath, format: str = None, chunksize: int = None):
        """
        :param vcfpath:
        :param indextype: identifier for variants: 'id', 'chrom:pos'.
        Must be 'chrom:pos' if the input has been generated by Phaser
        """
        self.path = vcfpath
        self.fmt = format
        self.chksz = chunksize
        self.chrom = [*pysam.VariantFile(self.path).header.contigs][0]
        # extract chrom, works if only 1 chrom in the file
        self.pack = True
        self.newpos = 1  # for valid self.newpos - 1 at the start of the first chunk

    def chunk(self, chunksize: int, newpos: int):
        """Build generators of variants calls"""
        iterator = pysam.VariantFile(self.path)
        try:
            for i, v in enumerate(iterator.fetch(contig=self.chrom, start=newpos - 1, reopen=False)):
                # newpos - 1: avoids first variant truncation in the next chunk
                var = v
                if i == chunksize:
                    break
                yield var

        except StopIteration:
            print('Could not build chunk')

    def incrementer(self, chunksize: int):
        """update position and packing bool"""
        iterator = pysam.VariantFile(self.path)
        try:
            for i, v in enumerate(iterator.fetch(contig=self.chrom, start=self.newpos - 1, reopen=False)):
                # self.newpos - 1: avoids first variant truncation in the next chunk
                var = v
                if i == chunksize:
                    self.newpos = var.pos
                    break
            if var.pos != self.newpos:  # reached EOF
                self.pack = False

        except StopIteration:
            self.newpos = None
            self.pack = False

        finally:
            return self.newpos, self.pack

    def chunkpacker(self):
        while self.pack:
            chk = self.chunk(self.chksz, self.newpos)
            # function output and included sttributes updates NOT unpacked hence NOT updated
            self.newpos, self.pack = self.incrementer(self.chksz)
            yield chk
            print(self.newpos, self.pack)


class PysamChunkPooler(object):
    """
    Class translation of pool.process_file
    modify header if GT pooled to GL
    what I will be able to do with chunks of variants:
    * track header of the main file, update header if necessary
    * process e.g. simualte pooling
    * write back to vcf
    """
    def __init__(self, mainpath: FilePath, packedchunk):
        self.mainf = pysam.VariantFile(mainpath)
        self.data = packedchunk

    @property
    def header(self):
        return self.mainf.header

    def process(self):
        # any way to update a record?
        for rec in self.data:
            var = rec
            for v in var.samples.values():
                v['GT'] = (None, None)
            yield var

    def writechunk(self, pathout: FilePath, data):
        """
        :param data: variant generator
        """
        fout = pysam.VariantFile(pathout, 'w', header=self.header)
        data = self.process()
        for rec in data:
            var = rec
            print([v['GT'] for v in var.samples.values()])
            fout.write(rec)
        fout.close()


class PysamVariantPooler(object):
    """
    Class translation of pool.process_line
    """
    def __init__(self, groups: list, simul: str, format: str,
                 w: pysam.VariantFile, v: pysam.VariantRecord,
                 dict_gl: dict,  write: bool = True):
        self.groups = groups
        self.simul = simul
        self.fmt = format
        self.writer = w
        self.record = v
        self.lookup = dict_gl
        self._write = write
        self.samples = self.record.samples.keys()
        self.genotypes = np.asarray(self.record.samples.values()[self.fmt])
        self.pooled_record = self.record.copy()
        self.blocks = []
        for gp in groups[0]:
            self.blocks.append(pool.SNPsPool().set_subset(gp))

    def simulate_pooling(self):
        self.pooled_record.format = prm.GTGL
        for p in self.blocks:
            p.set_line_values(self.samples, self.record)
            if prm.GTGL == 'GL' and prm.unknown_gl == 'adaptive':
                self.pooled_record = p.decode_genotypes_gp(self.genotypes, self.lookup)
            else:  # prm.GTGL == 'GT' or fixed GL
                self.pooled_record = p.decode_genotypes_gt(self.genotypes)

    def simulate_rdmissing(self):
        pass

    def write_variant(self):
        pass


class CyvcfVariantCallGenerator(object):
    """
    Generate variants calls from a file, starting at a chosen genetic position.
    Read GT-formatted VCF-files only (based on cyvcf2 library).
    """
    def __init__(self, vcfpath: FilePath) -> None:
        """
        :param vcfpath: path to VCF-file
        """
        self.path = vcfpath
        self.chrom = [*cyvcf2.VCF(self.path)][0].CHROM

    def __iter__(self, pos: str = None) -> cyvcf2.VCF:
        """
        :param pos: access variants from pos position
        """
        if pos is None:
            vcfobj = cyvcf2.VCF(self.path, lazy=True, threads=os.cpu_count())
        else:
            f = cyvcf2.VCF(self.path, lazy=True, threads=os.cpu_count())
            vcfobj = f('{}:{}'.format(self.chrom, pos))
        return vcfobj


class CyvcfVariantChunkGenerator(object):
    """
    Generate repacked chunks of variants calls from a file
    Based on cyvcf2 I/O functionalities.
    Ex a VCF-file containing 1,000 would be repacked (and rewritten) inot 10 files with 100 variants each.
    """

    def __init__(self, vcfpath: FilePath, chunksize: int = None) -> None:
        """
        :param vcfpath: path to VCF-file to repack
        :param chunksize: maximum number of variants per packed chunk. If None, no repacking is done
        """
        self.path = vcfpath
        self.chksz = chunksize
        self.chrom = [*cyvcf2.VCF(self.path, lazy=True, threads=os.cpu_count())][0].CHROM
        # extract chrom, works if only 1 chrom in the file
        self.pack = True
        self.newpos = 1  # for valid self.newpos - 1 at the start of the first chunk

    def _chunker(self, chunksize: int, newpos: int) -> Generator[cyvcf2.Variant, None, None]:
        """
        Build fixed-length generator of variants calls.
        Keep track of the position where chunking stops
        """
        iterator = cyvcf2.VCF(self.path, lazy=True, threads=os.cpu_count())
        try:
            for i, v in enumerate(iterator('{}:{}'.format(self.chrom, newpos - 1))):
                # newpos - 1: avoids first variant truncation in the next chunk
                var = v
                if i == chunksize:
                    break
                yield var

        except StopIteration:
            print('Could not build chunk')

    def _incrementer(self, chunksize: int) -> Tuple[int, bool]:
        """update position and packing bool"""
        iterator = cyvcf2.VCF(self.path, lazy=True, threads=os.cpu_count())
        try:
            for i, v in enumerate(iterator('{}:{}'.format(self.chrom, self.newpos - 1))):
                # self.newpos - 1: avoids first variant truncation in the next chunk
                var = v
                if i == chunksize:
                    self.newpos = var.POS  # POS in cap. letters with cyvcf2
                    break
            if var.POS != self.newpos:  # reached EOF
                self.pack = False

        except StopIteration:
            self.newpos = None
            self.pack = False

        finally:
            return self.newpos, self.pack

    def chunkpacker(self) -> Generator[cyvcf2.VCF, None, None]:
        while self.pack:
            chk = self._chunker(self.chksz, self.newpos)
            # function output and included sttributes updates NOT unpacked hence NOT updated
            self.newpos, self.pack = self._incrementer(self.chksz)
            yield chk
            # print(self.newpos, self.pack)

    def chunkwriter(self, chki: int, pathout: FilePath) -> None:
        """
        Write a packed chunk to its own file. Header of chunk file is copied from the parent file.
        """
        data = [*self.chunkpacker()][chki]
        w = cyvcf2.Writer(pathout, cyvcf2.VCF(self.path, lazy=True, threads=os.cpu_count()))
        for var in data:
            w.write_record(var)
        w.close()


class CyvcfChunkHandler(object):
    """
    Tools for processing a (chunked) VCF-file:
    * track header of the main file, update header if necessary
    * simualte pooling
    * write processed variants back to a VCF-file (Can write GT-formatted VCF-files only, work-around for GL)
     (based on cyvcf2 library).
    """
    def __init__(self, mainpath: FilePath, groups: list = None, gdict: dict = None,
                 fileout: FilePath = None,
                 packedchunk: Iterator[cyvcf2.Variant] = None) -> None:
        """
        :param fileout: path to an uncompressed VCF-file
        """
        self.mainf = cyvcf2.VCF(mainpath, lazy=True, threads=os.cpu_count())
        if packedchunk is not None:
            self.data = packedchunk
        else:
            self.data = self.mainf
        self.groups = groups
        self.gdict = gdict
        self.filout = fileout

    @property
    def header(self):
        return self.mainf.raw_header

    def add_format_to_header(self, line=None) -> None:
        """
        :param line: dict containing keys for ID, Number, Type, Description.
        """
        str_header = '##FORMAT=<ID=GL,Number=G,Type=Float,Description="three log10-scaled likelihoods for RR,RA,AA genotypes">'
        dic_header = {'ID': 'GL',
                      'Number': 'G',
                      'Type': 'Float',
                      'Description': 'three log10-scaled likelihoods for RR,RA,AA genotypes'}
        if line is None: line = dic_header
        self.mainf.add_format_to_header(line)

    def write_header(self) -> None:
        self.add_format_to_header()
        fout = cyvcf2.Writer(self.filout, self.mainf)
        fout.write_header()
        fout.close()

    def process(self):
        print('\r\nWriting metadata in {}'.format(self.filout).ljust(80, '.'))
        if False:  # write GT only format
            fout = cyvcf2.Writer(self.filout, self.mainf, mode='w')
        else:  # write other formats
            self.write_header()
            fout = open(self.filout, 'ab')
        print('Pooling and writing data in {}'.format(self.filout).ljust(80, '.'))
        for rec in self.data:
            var = rec
            cvp = CyvcfVariantPooler(self.groups, 'pooled', fout, var, self.gdict, write=True)
            cvp.simulate_pooling()
            cvp.write_variant()
            # break  process 1 variant only
        fout.close()
        print('Writing data in {}: Done'.format(self.filout).rjust(80, '.'))

    def write_chunk(self) -> None:
        """implement if self.process yields a generator over the data to write"""
        fout = cyvcf2.Writer(self.filout, self.mainf)
        fout.close()

    def bgzip_index_chunk(self) -> None:
        pybcf.bgzip(self.filout, self.filout + '.gz', os.getcwd())
        pybcf.index(self.filout + '.gz', os.getcwd())
        # clean uncompressed vcf file
        delete_file(self.filout)


class CyvcfVariantPooler(object):
    """
    Class translation of pool.process_line
    Based on cyvcf2 I/O functionalities.
    Pooling encoding and decoding inspired from DNA Sudoku and pattern consistency decoding.
    """
    def __init__(self, groups: list, simul: str,
                 w: cyvcf2.Writer, v: cyvcf2.Variant,
                 gdict: dict,  write: bool = True) -> None:
        self.groups = groups
        self.simul = simul
        self.writer = w
        self.record = v
        self.lookup = gdict
        self._write = write
        self.samples = cyvcf2.VCF(self.writer.name, lazy=True, threads=os.cpu_count()).samples
        self.genotypes = np.asarray(self.record.genotypes)
        self.pooled_record = self.record
        self.blocks = [pool.SNPsPool().set_subset(np.asarray(gp)) for gp in self.groups[0]]

    def simulate_pooling(self) -> None:
        for p in self.blocks:
            p.set_line_values(self.samples, self.record)
            if prm.GTGL == 'GL' and prm.unknown_gl == 'adaptive':
                self.pooled_record = p.decode_genotypes_gp(self.genotypes, self.lookup)
            else:  # prm.GTGL == 'GT' or fixed GL
                self.pooled_record = p.decode_genotypes_gt(self.genotypes)

    def simulate_rdmissing(self) -> None:
        for p in self.blocks:
            p.set_line_values(self.samples, self.record)
            dlt = pool.random_delete(activate=True)
            idx = np.argwhere(np.isin(self.samples, p))
            if dlt:
                if prm.GTGL == 'GL' and prm.unknown_gl == 'adaptive':
                    self.pooled_record = self.pooled_record.astype(float)  # avoid truncating GL
                    np.put(self.pooled_record, idx, np.asarray([1 / 3, 1 / 3, 1 / 3]))
                else:
                    np.put(self.pooled_record, idx, np.asarray([-1, -1, 0]))

    def write_variant(self) -> None:
        if prm.GTGL == 'GL' and prm.unknown_gl == 'adaptive':
            # cyvcf2.Variant.genotypes does not handle GL-format
            # customize line format for GL
            logzero = np.vectorize(lambda x: -5.0 if x <= pow(10, -5) else math.log10(x))
            info = ';'.join([kv for kv in ['='.join([str(k), str(v)]) for k, v in self.record.INFO]])
            gl = alltls.repr_gl_array(logzero(self.pooled_record))
            toshow = np.asarray([self.record.CHROM,
                                 self.record.POS,
                                 self.record.ID,
                                 ''.join(self.record.REF),
                                 ''.join(self.record.ALT),
                                 self.record.QUAL if not None else '.',
                                 'PASS' if self.record.FILTER is None else self.record.FILTER,
                                 info,
                                 'GL',
                                 gl],
                                dtype=str)
            towrite = '\t'.join(toshow) + '\n'
            stream = towrite.encode()
            self.writer.write(stream)  # cyvcf2.Writer.variant_from_string() does not write anything
        else:
            # cyvcf2.Variant.genotypes does handle GT-format
            self.record.genotypes = self.pooled_record.tolist()
            self.writer.write_record(self.record)


def cyvcfchunkhandler_process(*arglist):
    """
    Wrap class and methods into a function useable as multiprocessing input object.
    """
    start = time.time()
    cch = CyvcfChunkHandler(*arglist)
    cch.process()
    cch.bgzip_index_chunk()
    print('cyvcfchunkhandler_process() --> {} sec'.format(time.time() - start))


def pysamchunkhandler_process(*arglist):
    """
    See classes and pysam_pooler() in poolvcf.py
    """
    pass


if __name__ == '__main__':
    # tests #
    import time

    os.chdir('/home/camille/1000Genomes/data/gl/gl_adaptive/all_snps_all_samples')
    gtglpth = 'IMP.chr20.pooled.beagle2.gl.chunk10000.corr.vcf.gz'
    gtpth = '/home/camille/1000Genomes/data/gt/ALL.chr20.snps.gt.chunk10000.vcf.gz'

    pysamvar = PysamVariantCallGenerator(gtglpth, format='GP')
    pysamchunk = PysamVariantChunkGenerator(gtglpth, format='GP', chunksize=1000)

    cyvcfvar = CyvcfVariantCallGenerator(gtpth)
    cyvcfchunk = CyvcfVariantChunkGenerator(gtpth, chunksize=1000)

    df = pd.read_csv(os.path.join(prm.WD, 'adaptive_gls.csv'),
                     header=None,
                     names=['rowsrr', 'rowsra', 'rowsaa', 'colsrr', 'colsra', 'colsaa',
                            'n', 'm',
                            'rr', 'ra', 'aa']
                     )
    df2dict = dict(((int(rwrr), int(rwra), int(rwaa), int(clrr), int(clra), int(claa),
                     int(n), int(m)),
                    [rr, ra, aa]) for rwrr, rwra, rwaa, clrr, clra, claa,
                                      n, m,
                                      rr, ra, aa in df.itertuples(index=False, name=None))

    raw = cyvcf2.VCF(os.path.join(prm.WD, 'gt', prm.SRCFILE))  # VCF iterator object
    splits = pool.split_pools(raw.samples, 16, seed=123)  # list of lists

    # chunkpack = pysamchunk.chunkpacker()
    # for i, chk in enumerate(chunkpack):
    #     break
    #     # print(i, len([*chk]))  # empties chk!
    #     print('\r\n', i)
    #     chki = PysamChunkHandler(mainpth, chk)
    #     chki.writechunk('chktest{}.vcf'.format(i), chki.data)
    #     break

    cyvcfpack = cyvcfchunk.chunkpacker()
    # for i, chk in enumerate(cyvcfpack):
    #      # print(i, len([*chk]))
    #      chkdata = chk
    #      break
    #
    # cyvcfhandler = CyvcfChunkHandler(gtpth, chkdata, groups=splits, gdict=df2dict)
    # cyvcfhandler.process()  # processes 1 chunk

    # must write chunks to files before further processing
    start = time.time()
    prename = 'pack'
    indices = np.arange(len([*cyvcfpack]))
    baspath = os.path.basename(gtpth).rstrip('.gz')
    files0 = [os.path.join(prm.TMP_DATA_PATH,
                           '{}{}.{}'.format(prename, chki, baspath)) for chki in indices]
    args0 = list(zip(indices,
                     files0))
    with mp.Pool(processes=os.cpu_count()) as mpool:
       _ = all(mpool.starmap(CyvcfVariantChunkGenerator(gtpth, chunksize=1000).chunkwriter, args0))
    print('\r\nTime elapsed --> ', time.time() - start)  # 12.965112209320068

    files0 = os.listdir(prm.TMP_DATA_PATH)
    files1 = ['pooled.{}'.format(f0) for f0 in files0]
    args1 = list(zip(files0,
                     repeat(splits, len(indices)),
                     repeat(df2dict, len(indices)),
                     files1))
    os.chdir(prm.TMP_DATA_PATH)
    # cch = cyvcfchunkhandler_process(*args1[0])  # processes 1 chunked file
    with mp.Pool(processes=os.cpu_count()//2) as mpool:
         _ = all(mpool.starmap(cyvcfchunkhandler_process, args1))

    files2 = ['{}.gz'.format(f1) for f1 in files1]
    pybcf.concat(files2, 'ALL.chr20.snps.gt.chunk10000.vcf', prm.TMP_DATA_PATH)
    pybcf.sort('ALL.chr20.snps.gt.chunk10000.vcf', prm.TMP_DATA_PATH)
    pybcf.bgzip('ALL.chr20.snps.gt.chunk10000.vcf', 'ALL.chr20.snps.gt.chunk10000.vcf.gz', prm.TMP_DATA_PATH)
    pybcf.index('ALL.chr20.snps.gt.chunk10000.vcf.gz', prm.TMP_DATA_PATH)
    delete_file('ALL.chr20.snps.gt.chunk10000.vcf')
    print('\r\nTime elapsed --> ', time.time() - start)  # 19.328217029571533

